{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The Cooking Chef Problem"
      ],
      "metadata": {
        "id": "A9Ojq_5Jr_iJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the case where the agent is your personal Chef.\n",
        "In particular, the agent (the smiley on the map) wants to cook the eggs recipe according to your\n",
        "indication (scrambled or pudding).\n",
        "In order to cook the desired recipe, the agent must first collect the needed tools (the egg beater on the\n",
        "map). Then he must reach the stove (the frying pan or the oven on the map). Finally, he can cook.\n",
        "Not that there are two special interlinked cells (marked with the G) that allow the agent to go from\n",
        "one side of the map to the other. But to do so, the agent needs to express his will to go on the other\n",
        "side.\n",
        "Cells in (4, 2) and (9, 3) are the special gate ones. They allow the agent to go from one side of the\n",
        "map to another. Those two special cells are interlinked, but the agent needs to express his will to go\n",
        "on the other side.\n",
        "Since you are very hungry, it is fundamental that the agent cooks the eggs according to your taste\n",
        "(scrambled/pudding) as fast as he can without letting you wait for more than necessary.\n",
        "In order to apply optimal control techniques such as value iteration, you need to model the aforementioned scenario as an MDP. Recall that an MDP is defined as a tuple (S, A, P, R, γ), where:\n",
        "S: The (finite) set of all possible states.\n",
        "A: The (finite) set of all possible actions.\n",
        "P: The transition function P : S × S × A → [0, 1], which maps (s\n",
        "0\n",
        ", s, a) to P(s\n",
        "0\n",
        "|s, a), i.e., the\n",
        "probability of transitioning to state s\n",
        "0 ∈ S when taking action a ∈ A in state s ∈ S. Note\n",
        "that P\n",
        "s\n",
        "0∈S P(s\n",
        "0\n",
        "|s, a) = 1 for all s ∈ S, a ∈ A.\n",
        "R: The reward function R : S × A × S → R, which maps (s, a, s0\n",
        ") to R(s, a, s0\n",
        "), i.e., the reward\n",
        "obtained when taking action a ∈ A in state s ∈ S and arriving at state s\n",
        "0 ∈ S.\n",
        "γ: The discount factor which controls how important rewards are in the future.\n",
        "In order to encode this problem as an MDP, you need to define each of the components of the tuple\n",
        "for our particular problem. Note that there may be many different possible encodings (specify them\n",
        "in the report)."
      ],
      "metadata": {
        "id": "UqEpNMqRsGI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, considering the problem as a model-free scenario, provide a program (written in Python,\n",
        "possibly based on the labs) that can compute the optimal policy for this world by solely considering\n",
        "the pudding eggs scenario. Draw the computed policy in the grid by putting the optimal action in\n",
        "each cell. If multiple actions are possible, include the probability of each arrow. There may be\n",
        "multiple optimal policies; pick one to show it. Note that the model is not available for computation\n",
        "but must be encoded to be used as the \"real-world\" environment."
      ],
      "metadata": {
        "id": "Gdrbfj_qr5K6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGJsMVxxgXtS",
        "outputId": "9c81199e-440d-48a9-a3ad-f1a6a1e704de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "↑ ← ← ↑ ↓ ↑ ← ← → ←\n",
            "↓ █ █ █ F ← ← → → ←\n",
            "← █ ↓ █ ↑ ↑ ← → → ←\n",
            "↓ █ → → ↑ ← ← ↓ → ←\n",
            "↓ → G ↑ ↑ ← ← → ← ←\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world\n",
        "grid_height = 5\n",
        "grid_width = 10\n",
        "start_state = (4, 3)\n",
        "egg_beaters = [(1, 3), (8, 3)]\n",
        "goals = [(1, 4), (8, 4)]\n",
        "gate_cells = [(4, 2), (9, 3)]\n",
        "walls = [(1, 1), (2, 1), (3, 1), (5, 1), (6, 1), (7, 1), (1, 2), (7, 2), (1, 3), (2, 3), (6, 3), (7, 3)]\n",
        "\n",
        "# Define actions\n",
        "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]  # Left, Right, Up, Down\n",
        "\n",
        "# Define parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n",
        "\n",
        "# Initialize Q-values\n",
        "Q = np.zeros((grid_height, grid_width, len(actions)))\n",
        "\n",
        "# Helper function to check if a state is valid\n",
        "def is_valid_state(state):\n",
        "    x, y = state\n",
        "    return 0 <= x < grid_height and 0 <= y < grid_width and state not in walls\n",
        "\n",
        "# Helper function to get the next state after taking an action\n",
        "def get_next_state(state, action):\n",
        "    next_state = (state[0] + action[0], state[1] + action[1])\n",
        "    if is_valid_state(next_state):\n",
        "        return next_state\n",
        "    return state\n",
        "\n",
        "# Reward function\n",
        "def get_reward(state, action, next_state):\n",
        "    reward = -1  # Default reward for each step\n",
        "    if next_state in egg_beaters:\n",
        "        reward += 10  # Additional reward for reaching egg beater\n",
        "    if next_state in goals:\n",
        "        reward += 100  # Additional reward for reaching goal\n",
        "    return reward\n",
        "\n",
        "# Q-learning algorithm\n",
        "def q_learning():\n",
        "    num_episodes = 1000\n",
        "    for _ in range(num_episodes):\n",
        "        state = start_state\n",
        "        while state not in goals:\n",
        "            action_index = choose_action(state)\n",
        "            action = actions[action_index]\n",
        "            next_state = get_next_state(state, action)\n",
        "            reward = get_reward(state, action, next_state)\n",
        "            next_action_max = np.max(Q[next_state[0], next_state[1]])\n",
        "            Q[state[0], state[1], action_index] += alpha * (\n",
        "                    reward + gamma * next_action_max - Q[state[0], state[1], action_index])\n",
        "            state = next_state\n",
        "\n",
        "# Helper function to choose an action based on epsilon-greedy policy\n",
        "def choose_action(state):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(len(actions))\n",
        "    else:\n",
        "        return np.argmax(Q[state[0], state[1]])\n",
        "\n",
        "# Compute optimal policy\n",
        "q_learning()\n",
        "\n",
        "# Print optimal policy\n",
        "policy_symbols = ['←', '→', '↑', '↓']\n",
        "policy = np.empty((grid_height, grid_width), dtype=str)\n",
        "for i in range(grid_height):\n",
        "    for j in range(grid_width):\n",
        "        if (i, j) in walls:\n",
        "            policy[i, j] = '█'  # Wall symbol\n",
        "        elif (i, j) in gate_cells:\n",
        "            policy[i, j] = 'G'  # Gate symbol\n",
        "        elif (i, j) in egg_beaters:\n",
        "            policy[i, j] = 'B'  # Egg beater symbol\n",
        "        elif (i, j) in goals:\n",
        "            policy[i, j] = 'F'  # Goal symbol\n",
        "        else:\n",
        "            best_action = np.argmax(Q[i, j])\n",
        "            policy[i, j] = policy_symbols[best_action]\n",
        "\n",
        "# Display the computed policy\n",
        "for row in policy:\n",
        "    print(' '.join(row))\n"
      ]
    }
  ]
}